### Default config
mode: finetune
seed: 0 # fix as 0 in pretrain
gpu: [0]
num_workers: 1
dist_url: tcp://localhost:7997
episodes: 1

### Dataset config
dataset_name: pamap2
train_dataset_path: /mnt/sting/hjyoon/projects/cross/PAMAP2/augcon/target_domain_wrist/finetune/10shot/target/train.pkl
test_dataset_path: /mnt/sting/hjyoon/projects/cross/PAMAP2/augcon/target_domain_wrist/finetune/10shot/target/test.pkl
val_dataset_path: /mnt/sting/hjyoon/projects/cross/PAMAP2/augcon/target_domain_wrist/finetune/10shot/target/val.pkl
input_channels: 3
num_cls: 12

### Training config
optimizer: adam
criterion: crossentropy
start_epoch: 0
epochs: 50
batch_size: 4
lr: 0.001
wd: 0.0

### Logs and checkpoints
resume: ''
pretrained: ./temp/checkpoint_0099.pth.tar
ckpt_dir: ./temp/finetune
log_freq: 5
save_freq: 10

### Model config
pretext: autoencoder
#For tpn
out_dim: 2
T: 0.1
z_dim: 256
### Meta-learning
domain_adaptation: false
task_steps: 10
task_lr: 0.001
reg_lambda: 0
no_vars: true
mlp: false
freeze: true

