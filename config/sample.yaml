### Default config
mode: finetune
seed: 0 # fix as 0 in pretrain
gpu: [7]
num_workers: 1
dist_url: tcp://localhost:6007
episodes: 1

### Dataset config
dataset_name: ichar
train_dataset_path: /mnt/sting/hjyoon/projects/cross/ICHAR/augcon/target_domain_WA0002-bkkim/finetune/10shot/target/train.pkl
test_dataset_path: /mnt/sting/hjyoon/projects/cross/ICHAR/augcon/target_domain_WA0002-bkkim/finetune/10shot/target/test.pkl
val_dataset_path: /mnt/sting/hjyoon/projects/cross/ICHAR/augcon/target_domain_WA0002-bkkim/finetune/10shot/target/val.pkl
input_channels: 3
num_cls: 9

### Training config
optimizer: adam
criterion: crossentropy
start_epoch: 0
epochs: 50
batch_size: 4
lr: 0.001
wd: 0.0

### Logs and checkpoints
resume: ''
pretrained: ./temp/checkpoint_0099.pth.tar
ckpt_dir: ./temp/finetune
log_freq: 5
save_freq: 10

### Model config
pretext: metasimclr


### Meta-learning
domain_adaptation: false

#For simclr
out_dim: 50
T: 0.1
z_dim: 256
 
task_steps: 10
task_lr: 0.001
reg_lambda: 0
no_vars: true
mlp: false
freeze: true

